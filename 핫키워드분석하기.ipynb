{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlwt\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib import request, parse\n",
    "from selenium import webdriver\n",
    "from settings import WEB_DRIVER_PATH\n",
    "import re\n",
    "\n",
    "# 변수들\n",
    "district_names = ['서경대 핫', '홍대입구 핫', '건대입구 핫', '신촌 핫', '강남역 핫', '이태원 핫']\n",
    "\n",
    "years = []\n",
    "FIRST_DAY = '0101'\n",
    "LAST_DAY = '1231'\n",
    "START_YEAR = 2007\n",
    "END_YEAR = 2018\n",
    "\n",
    "# 기본 셋팅\n",
    "def make_years(start_year, end_year, years):\n",
    "    for year in range(start_year, end_year+1):\n",
    "        years.append(str(year))\n",
    "\n",
    "make_years(START_YEAR, END_YEAR, years)\n",
    "driver = webdriver.Chrome(WEB_DRIVER_PATH)\n",
    "wb = xlwt.Workbook()\n",
    "ws = wb.add_sheet('blogs')\n",
    "\n",
    "base_url = \"search.naver.com/search.naver?where=post&query={0}&st=sim&sm=tab_opt&date_from={1}&date_to={2}&date_to=20151231&date_option=8&srchby=all&dup_remove=1\"\n",
    "def crawl(district_names, years):\n",
    "    row = 0;\n",
    "    for district in district_names:\n",
    "        print(district)\n",
    "        posting_nums = []\n",
    "        for index, year in enumerate(years):\n",
    "            # make url\n",
    "            start_day = year + FIRST_DAY\n",
    "            end_day = year + LAST_DAY\n",
    "            url = 'https://' + base_url.format(district, start_day, end_day)\n",
    "\n",
    "            # 갯수 크롤링해서\n",
    "            num = get_posting_num(url)\n",
    "            posting_nums.append(num)\n",
    "\n",
    "        # 엑셀에 넣기\n",
    "        row = save_xlsx(district, posting_nums, ws, row)\n",
    "    wb.save('C:/workspace/data_analysis/real_estimate_rent/num_of_postings_golmok.xls')\n",
    "\n",
    "def save_xlsx(district_name, num_of_postings, ws, row):\n",
    "    print('save_xlsx')\n",
    "    ws.write(row, 0, district_name)\n",
    "    for index, num in enumerate(num_of_postings):\n",
    "        ws.write(row, index+1, num)\n",
    "    return row + 1\n",
    "\n",
    "def get_posting_num(url):\n",
    "    # 건수에 해당하는 스트링 가져오기\n",
    "    driver.get(url)\n",
    "    html = driver.page_source\n",
    "    bs = BeautifulSoup(html, 'html5lib')\n",
    "    posting_nums = bs.select('.title_num')\n",
    "\n",
    "    # 건수만 가져오기\n",
    "    # print(posting_nums)\n",
    "    if len(posting_nums) == 0:\n",
    "        num = 0\n",
    "    else:\n",
    "        num = re.findall(r'([\\d|\\,]+)건', str(posting_nums))[0]\n",
    "        num = num.replace(',', '')\n",
    "\n",
    "    return num\n",
    "\n",
    "crawl(district_names, years)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
